#!/usr/bin/env python
# -*- coding: utf-8 -*-
# src/data_collector/polling_data_collector.py

import json
import logging
import threading
import time
from datetime import UTC, datetime

import ccxt
import pytz
import redis

from HYPERRSI.src.config import OKX_API_KEY, OKX_PASSPHRASE, OKX_SECRET_KEY
from HYPERRSI.src.core.config import settings
from HYPERRSI.src.trading.models import get_auto_trend_timeframe
from shared.indicators import compute_all_indicators, add_auto_trend_state_to_candles
from shared.logging import get_logger

# ë¡œê¹… ì„¤ì •
logger = get_logger(__name__)
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')

# ì„¤ì • ë° ìƒìˆ˜
SYMBOLS = ["BTC-USDT-SWAP", "ETH-USDT-SWAP", "SOL-USDT-SWAP"]
TIMEFRAMES = [1, 3, 5, 15, 30, 60, 240]  # ë¶„ ë‹¨ìœ„
TF_MAP = {1: '1m', 3: '3m', 5: '5m', 15: '15m', 30: '30m', 60: '1h', 240: '4h'}
MAX_CANDLE_LEN = 3000
POLLING_CANDLES = 10  # í•œ ë²ˆì— í´ë§í•  ìº”ë“¤ ìˆ˜ (ë°” ì¢…ë£Œ ì‹œì ì— ìµœì‹  ëª‡ ê°œë§Œ í™•ì¸)
MIN_CANDLES_FOR_INDICATORS = 199  # ì§€í‘œ ê³„ì‚°ì— í•„ìš”í•œ ìµœì†Œ ìº”ë“¤ ìˆ˜ (SMA200ì€ ì¸ë±ìŠ¤ 199ë¶€í„° ì •í™•í•˜ê²Œ ê³„ì‚°ë¨)

# ì—­ë§¤í•‘ ìƒì„± (ex: '1m' -> 1)
REVERSE_TF_MAP = {v: k for k, v in TF_MAP.items()}



# Redis í´ë¼ì´ì–¸íŠ¸ ì„¤ì • - Use shared sync Redis connection pool
from shared.database.redis import RedisConnectionManager

redis_manager = RedisConnectionManager(
    host=settings.REDIS_HOST,
    port=settings.REDIS_PORT,
    db=0,
    password=settings.REDIS_PASSWORD if settings.REDIS_PASSWORD else None
)
redis_client = redis_manager.get_connection()

# OKX API ì„¤ì •
OKX_API_KEY = OKX_API_KEY
OKX_SECRET = OKX_SECRET_KEY
OKX_PASSPHRASE = OKX_PASSPHRASE

exchange = ccxt.okx({
    'apiKey': OKX_API_KEY,
    'secret': OKX_SECRET,
    'password': OKX_PASSPHRASE,
    'enableRateLimit': True,
    'timeout': 30000,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ (ë„¤íŠ¸ì›Œí¬ ì§€ì—° ëŒ€ì‘)
    'options': {
        'defaultType': 'swap',
        'adjustForTimeDifference': True,  # ì„œë²„ ì‹œê°„ ì°¨ì´ ìë™ ì¡°ì •
        'recvWindow': 10000,  # ìš”ì²­ ìˆ˜ì‹  ìœˆë„ìš° 10ì´ˆ
    }
})






# ì•ˆì „í•œ ì¢…ë£Œë¥¼ ìœ„í•œ ì´ë²¤íŠ¸ ê°ì²´
shutdown_event = threading.Event()

# ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì™„ë£Œ í”Œë˜ê·¸
initial_data_loaded = threading.Event()

# ë§ˆì§€ë§‰ ìº”ë“¤ íƒ€ì„ìŠ¤íƒ¬í”„ ë° ë§ˆì§€ë§‰ ì²´í¬ ì‹œê°„ ì €ì¥
last_candle_timestamps: dict[str, int] = {}
last_check_times: dict[str, float] = {}

from shared.utils.time_helpers import align_timestamp, calculate_update_interval, is_bar_end

# CandlesDB Writer
from HYPERRSI.src.data_collector.candlesdb_writer import get_candlesdb_writer
candlesdb_writer = get_candlesdb_writer()


def _get_candles_from_redis_for_auto_trend(symbol: str, tf_str: str) -> list:
    """
    Redisì—ì„œ auto_trend_state ê³„ì‚°ìš© ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    Args:
        symbol: ì‹¬ë³¼ (ì˜ˆ: "BTC-USDT-SWAP")
        tf_str: íƒ€ì„í”„ë ˆì„ ë¬¸ìì—´ (ì˜ˆ: "30m", "1h")

    Returns:
        ìº”ë“¤ ë¦¬ìŠ¤íŠ¸ (timestamp, close í¬í•¨)
    """
    key = f"candles_with_indicators:{symbol}:{tf_str}"
    try:
        existing_list = redis_client.lrange(key, 0, -1)
        candles = []
        for item in existing_list:
            try:
                item_str = item.decode('utf-8') if isinstance(item, bytes) else item
                obj = json.loads(item_str)
                if "timestamp" in obj and "close" in obj:
                    candles.append(obj)
            except Exception:
                pass
        return candles
    except Exception as e:
        logger.warning(f"Redisì—ì„œ auto_trend ìº”ë“¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {symbol} {tf_str} - {e}")
        return []


def fetch_latest_candles(symbol, timeframe, limit=POLLING_CANDLES, include_current=False):
    """
    ìµœì‹  ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    OKX APIëŠ” ìµœëŒ€ 300ê°œê¹Œì§€ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ, limitì´ 300ë³´ë‹¤ í¬ë©´ ì—¬ëŸ¬ ë²ˆ ìš”ì²­
    """
    tf_str = TF_MAP.get(timeframe, "1m")
    logger.debug(f"ìµœì‹  ìº”ë“¤ í´ë§: {symbol} {tf_str} - {limit}ê°œ ìš”ì²­ (í˜„ì¬ ì§„í–‰ ìº”ë“¤ í¬í•¨: {include_current})")

    try:
        OKX_MAX_LIMIT = 300  # OKX API ìµœëŒ€ limit
        all_candles = []

        # limitì´ 300 ì´í•˜ë©´ í•œ ë²ˆë§Œ ìš”ì²­
        if limit <= OKX_MAX_LIMIT:
            ohlcvs = _fetch_ohlcv_with_retry(symbol, tf_str, limit, None)
            if not ohlcvs:
                return []
            all_candles = _parse_ohlcv_data(symbol, tf_str, timeframe, ohlcvs, include_current)
        else:
            # limitì´ 300ë³´ë‹¤ í¬ë©´ ì—¬ëŸ¬ ë²ˆ ìš”ì²­
            # OKX APIëŠ” sinceë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´í›„ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ê³¼ê±°ë¡œ ê°€ë ¤ë©´ ë‹¤ë¥¸ ë°©ì‹ í•„ìš”
            total_batches = (limit + OKX_MAX_LIMIT - 1) // OKX_MAX_LIMIT  # ceil division
            logger.info(f"ì´ {total_batches}ë²ˆì˜ ë°°ì¹˜ ìš”ì²­ ì˜ˆì •: {symbol} {tf_str}")

            for batch_num in range(total_batches):
                batch_limit = min(limit - len(all_candles), OKX_MAX_LIMIT)
                if batch_limit <= 0:
                    break

                # since ê³„ì‚°: ì´ë¯¸ ê°€ì§„ ê°€ì¥ ì˜¤ë˜ëœ ìº”ë“¤ë³´ë‹¤ ë” ê³¼ê±°
                if all_candles:
                    # ê°€ì¥ ì˜¤ë˜ëœ ìº”ë“¤ì˜ timestamp (ì´ˆ ë‹¨ìœ„)
                    oldest_ts = min(c["timestamp"] for c in all_candles)
                    # íƒ€ì„í”„ë ˆì„ ê¸¸ì´ë¥¼ ê³ ë ¤í•´ì„œ ê·¸ ì´ì „ ì‹œì  ê³„ì‚°
                    tf_seconds = timeframe * 60
                    since = (oldest_ts - batch_limit * tf_seconds) * 1000  # milliseconds
                    logger.info(f"ìº”ë“¤ ë°°ì¹˜ #{batch_num+1} ìš”ì²­: {symbol} {tf_str} - {batch_limit}ê°œ (since: {datetime.fromtimestamp(since/1000)})")
                else:
                    # ì²« ìš”ì²­ì€ ìµœì‹ ë¶€í„°
                    since = None
                    logger.info(f"ìº”ë“¤ ë°°ì¹˜ #{batch_num+1} ìš”ì²­: {symbol} {tf_str} - {batch_limit}ê°œ (ìµœì‹ ë¶€í„°)")

                ohlcvs = _fetch_ohlcv_with_retry(symbol, tf_str, batch_limit, since)
                if not ohlcvs:
                    logger.warning(f"ìº”ë“¤ ë°°ì¹˜ ìš”ì²­ ì‹¤íŒ¨: {symbol} {tf_str}")
                    break

                batch_candles = _parse_ohlcv_data(symbol, tf_str, timeframe, ohlcvs, include_current)
                if not batch_candles:
                    logger.warning(f"íŒŒì‹±ëœ ìº”ë“¤ ì—†ìŒ: {symbol} {tf_str}")
                    break

                # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ë³‘í•©
                added_count = 0
                for candle in batch_candles:
                    if not any(c["timestamp"] == candle["timestamp"] for c in all_candles):
                        all_candles.append(candle)
                        added_count += 1

                logger.info(f"ë°°ì¹˜ #{batch_num+1} ì™„ë£Œ: {added_count}ê°œ ìƒˆ ìº”ë“¤ ì¶”ê°€ (ì´ {len(all_candles)}ê°œ)")

                # ìƒˆë¡œ ì¶”ê°€ëœ ìº”ë“¤ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨ (ë” ì´ìƒ ê³¼ê±° ë°ì´í„° ì—†ìŒ)
                if added_count == 0:
                    logger.info(f"ë” ì´ìƒ ê°€ì ¸ì˜¬ ìº”ë“¤ ì—†ìŒ: {symbol} {tf_str}")
                    break

                # ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±í•˜ë©´ ì¤‘ë‹¨
                if len(all_candles) >= limit:
                    logger.info(f"ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±: {symbol} {tf_str} - {len(all_candles)}ê°œ")
                    break

                # API rate limit ê³ ë ¤
                time.sleep(0.5)

            # ì‹œê°„ìˆœ ì •ë ¬
            all_candles.sort(key=lambda x: x["timestamp"])
            logger.info(f"ì´ {len(all_candles)}ê°œ ìº”ë“¤ ìˆ˜ì§‘ ì™„ë£Œ: {symbol} {tf_str}")

        return all_candles

    except Exception as e:
        logger.error(f"ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {symbol} {tf_str} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CandleDataFetchError",
            severity="ERROR",
            symbol=symbol,
            metadata={"timeframe": tf_str, "component": "integrated_data_collector.fetch_latest_candles"}
        )
        return []


def _fetch_ohlcv_with_retry(symbol, tf_str, limit, since):
    """OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    max_retries = 5
    attempt = 0

    while True:
        try:
            params = {'instType': 'SWAP'}
            logger.debug(f"API ìš”ì²­: symbol={symbol}, timeframe={tf_str.lower()}, limit={limit}, since={since}")

            if since is None:
                ohlcvs = exchange.fetch_ohlcv(
                    symbol,
                    timeframe=tf_str.lower(),
                    limit=limit,
                    params=params
                )
            else:
                ohlcvs = exchange.fetch_ohlcv(
                    symbol,
                    timeframe=tf_str.lower(),
                    since=since,
                    limit=limit,
                    params=params
                )
            return ohlcvs

        except ccxt.RateLimitExceeded as e:
            attempt += 1
            if attempt >= max_retries:
                logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {symbol} ({tf_str}). ì˜¤ë¥˜: {e}")
                raise e
            wait_time = 2 ** attempt
            logger.warning(f"ì†ë„ ì œí•œ ì´ˆê³¼: {symbol} ({tf_str}). {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... (ì‹œë„ {attempt}/{max_retries})")
            time.sleep(wait_time)
        except Exception as e:
            logger.error(f"OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {symbol} ({tf_str}). ì˜¤ë¥˜: {e}")
            # errordb ë¡œê¹…
            from HYPERRSI.src.utils.error_logger import log_error_to_db
            log_error_to_db(
                error=e,
                error_type="OHLCVFetchError",
                severity="WARNING",
                symbol=symbol,
                metadata={"timeframe": tf_str, "limit": limit, "since": since, "component": "integrated_data_collector._fetch_ohlcv_with_retry"}
            )
            return []


def _parse_ohlcv_data(symbol, tf_str, timeframe, ohlcvs, include_current):
    """OHLCV ë°ì´í„° íŒŒì‹±"""
    candles = []
    for row in ohlcvs:
        # None ê°’ ì²´í¬ ì¶”ê°€
        if row is None or len(row) < 6:
            logger.warning(f"ì˜ëª»ëœ ìº”ë“¤ ë°ì´í„° (None ë˜ëŠ” ë¶ˆì™„ì „): {symbol} {tf_str}")
            continue

        try:
            ts, o, h, l, c, v = row
            # None ê°’ íƒ€ì… ì²´í¬ ë° ë³€í™˜
            if ts is None or o is None or h is None or l is None or c is None or v is None:
                logger.warning(f"ìº”ë“¤ ë°ì´í„°ì— None ê°’ í¬í•¨: {symbol} {tf_str} - {row}")
                continue

            ts = int(ts) if ts is not None else 0
            aligned_ts = align_timestamp(ts, timeframe) // 1000

            # ë³¼ë¥¨ì´ 0ì¸ ìº”ë“¤ ì œì™¸ (ë‹¨, í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ì€ í—ˆìš©)
            is_current_candle = (aligned_ts + timeframe * 60) > int(time.time())

            if v == 0 and not is_current_candle:
                logger.warning(f"ë³¼ë¥¨ 0 ìº”ë“¤ ì œì™¸: {symbol} {tf_str} at {datetime.fromtimestamp(aligned_ts)}")
                continue

            candles.append({
                "timestamp": aligned_ts,
                "open": float(o),
                "high": float(h),
                "low": float(l),
                "close": float(c),
                "volume": float(v),
                "is_current": is_current_candle
            })
        except (TypeError, ValueError) as e:
            logger.warning(f"ìº”ë“¤ ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {symbol} {tf_str} - {row} - {e}")
            continue

    if candles:
        # ìº”ë“¤ì´ ì‹œê°„ìˆœ ì •ë ¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì •ë ¬
        candles.sort(key=lambda x: x["timestamp"])

        # ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ ì‹œê°„ ì €ì¥
        key = f"{symbol}:{tf_str}"

        completed_candles = [c for c in candles if not c.get("is_current", False)]
        if completed_candles:
            last_ts = completed_candles[-1]["timestamp"]
            old_last_ts = last_candle_timestamps.get(key, 0)

            if last_ts > old_last_ts:
                last_candle_timestamps[key] = last_ts
                logger.info(f"ë§ˆì§€ë§‰ ì™„ë£Œëœ ìº”ë“¤ íƒ€ì„ìŠ¤íƒ¬í”„ ì—…ë°ì´íŠ¸: {key} - {datetime.fromtimestamp(last_ts)}")

    return candles

def check_and_fill_gap(symbol, timeframe):
    """ë°ì´í„° ê°­ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì±„ìš°ê¸°"""
    tf_str = TF_MAP.get(timeframe, "1m")
    key = f"{symbol}:{tf_str}"
    
    try:
        # Redisì—ì„œ ê¸°ì¡´ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸°
        candle_key = f"candles:{symbol}:{tf_str}"
        existing_data = redis_client.lrange(candle_key, 0, -1)
        
        if not existing_data:
            logger.warning(f"ê¸°ì¡´ ë°ì´í„° ì—†ìŒ, ê°­ ì²´í¬ ë¶ˆê°€: {key}")
            return
        
        # ë§ˆì§€ë§‰ ìº”ë“¤ íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸
        latest_candles = fetch_latest_candles(symbol, timeframe, limit=1)
        if not latest_candles:
            logger.warning(f"ìµœì‹  ìº”ë“¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê°­ ì²´í¬ ë¶ˆê°€: {key}")
            return
            
        latest_ts = latest_candles[0]["timestamp"]
        
        # ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ ì°¾ê¸°
        existing_map = {}
        for item in existing_data:
            # Redis returns bytes, decode to string first
            item_str = item.decode('utf-8') if isinstance(item, bytes) else item
            parts = item_str.split(",")
            ts = int(parts[0])
            existing_map[ts] = parts
        
        existing_ts = sorted(existing_map.keys())
        last_existing_ts = existing_ts[-1] if existing_ts else 0
        
        # ê°­ ì²´í¬
        tf_minutes = timeframe
        expected_interval = tf_minutes * 60
        
        if (latest_ts - last_existing_ts) > expected_interval * 1.5:
            gap_size = latest_ts - last_existing_ts
            num_missing = int(gap_size / expected_interval)
            logger.info(
                f"ìº”ë“¤ ê°­ ë°œê²¬: {key} - "
                f"ë§ˆì§€ë§‰ ê¸°ì¡´: {datetime.fromtimestamp(last_existing_ts)}, "
                f"ìµœì‹ : {datetime.fromtimestamp(latest_ts)}, "
                f"ëˆ„ë½ëœ ìº”ë“¤ ìˆ˜: {num_missing}"
            )
            
            # ê°­ ì±„ìš°ê¸°
            fill_gap(symbol, timeframe, last_existing_ts, latest_ts)
    
    except Exception as e:
        logger.error(f"ê°­ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {key} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CandleGapCheckError",
            severity="WARNING",
            symbol=symbol,
            metadata={"timeframe": tf_str, "component": "integrated_data_collector.check_and_fill_gap"}
        )

def fill_gap(symbol, timeframe, from_ts, to_ts):
    """ë°ì´í„° ê°­ ì±„ìš°ê¸°"""
    tf_str = TF_MAP.get(timeframe, "1m")
    key = f"{symbol}:{tf_str}"
    
    try:
        logger.info(f"ê°­ ì±„ìš°ê¸° ì‹œì‘: {key} - {datetime.fromtimestamp(from_ts)} ~ {datetime.fromtimestamp(to_ts)}")
        
        # ê°­ì´ ë„ˆë¬´ í° ê²½ìš° ì œí•œ
        tf_minutes = timeframe
        expected_candles = (to_ts - from_ts) // (tf_minutes * 60)
        
        if expected_candles > 1000:
            logger.warning(f"ê°­ì´ ë„ˆë¬´ í½ë‹ˆë‹¤ ({expected_candles}ê°œ ìº”ë“¤), ìµœëŒ€ 1000ê°œë§Œ ìš”ì²­: {key}")
            from_ts = to_ts - (1000 * tf_minutes * 60)
        
        # APIë¡œ ê°­ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        params = {'instType': 'SWAP'}
        ohlcvs = exchange.fetch_ohlcv(
            symbol,
            timeframe=tf_str.lower(),
            since=(from_ts + 1) * 1000,  # +1ì´ˆ í•´ì„œ ë§ˆì§€ë§‰ ìº”ë“¤ ì¤‘ë³µ ë°©ì§€
            limit=1000,
            params=params
        )
        
        gap_candles = []
        for row in ohlcvs:
            ts, o, h, l, c, v = row
            aligned_ts = align_timestamp(ts, timeframe) // 1000
            
            # ë³¼ë¥¨ì´ 0ì¸ ìº”ë“¤ ì œì™¸
            if v == 0:
                continue
                
            gap_candles.append({
                "timestamp": aligned_ts,
                "open": float(o),
                "high": float(h),
                "low": float(l),
                "close": float(c),
                "volume": float(v)
            })
        
        if not gap_candles:
            logger.warning(f"ê°­ ë°ì´í„° ì—†ìŒ: {key}")
            return
            
        logger.info(f"{len(gap_candles)}ê°œ ê°­ ìº”ë“¤ ê°€ì ¸ì˜´: {key}")
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ì €ì¥
        update_candle_data(symbol, timeframe, gap_candles)
        
    except Exception as e:
        logger.error(f"ê°­ ì±„ìš°ê¸° ì¤‘ ì˜¤ë¥˜: {key} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CandleGapFillError",
            severity="WARNING",
            symbol=symbol,
            metadata={"timeframe": tf_str, "from_ts": from_ts, "to_ts": to_ts, "component": "integrated_data_collector.fill_gap"}
        )

def update_candle_data(symbol, timeframe, new_candles, warm_up_count=0):
    """
    ìº”ë“¤ ë°ì´í„° ì—…ë°ì´íŠ¸

    Args:
        symbol: ì‹¬ë³¼
        timeframe: íƒ€ì„í”„ë ˆì„
        new_candles: ìƒˆ ìº”ë“¤ ë¦¬ìŠ¤íŠ¸
        warm_up_count: ì§€í‘œ ê³„ì‚°ìš© warm-up ìº”ë“¤ ê°œìˆ˜ (ì´ ê°œìˆ˜ë§Œí¼ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ)
    """
    tf_str = TF_MAP.get(timeframe, "1m")
    key = f"candles:{symbol}:{tf_str}"

    try:
        # ê¸°ì¡´ ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        existing = redis_client.lrange(key, 0, -1)
        candle_map = {}

        # ê¸°ì¡´ ë°ì´í„° íŒŒì‹±
        for item in existing:
            # Redis returns bytes, decode to string first
            item_str = item.decode('utf-8') if isinstance(item, bytes) else item
            parts = item_str.split(",")
            ts = int(parts[0])
            candle_map[ts] = parts

        # ìƒˆ ìº”ë“¤ ë°ì´í„° ë³‘í•©
        for candle in new_candles:
            ts = candle["timestamp"]
            cndl_str_list = [
                str(ts),
                str(candle["open"]),
                str(candle["high"]),
                str(candle["low"]),
                str(candle["close"]),
                str(candle["volume"]),
            ]
            candle_map[ts] = cndl_str_list

        # ì •ë ¬
        sorted_ts = sorted(candle_map.keys())

        # warm_up_countê°€ ì§€ì •ëœ ê²½ìš°, ì²˜ìŒ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ ì œì™¸ (ì§€í‘œ ê³„ì‚°ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©)
        if warm_up_count > 0 and len(sorted_ts) > warm_up_count:
            logger.info(f"Warm-up ë°ì´í„° ì œì™¸: {symbol} {tf_str} - ì²˜ìŒ {warm_up_count}ê°œ ìº”ë“¤ì€ ì§€í‘œ ê³„ì‚°ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©")
            # ë‚˜ì¤‘ì— ì§€í‘œ ê³„ì‚° í›„ ì œì™¸í•  ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì „ì²´ ìœ ì§€

        # ìµœëŒ€ MAX_CANDLE_LENê°œë§Œ ìœ ì§€ (warm_up ì œì™¸í•˜ê¸° ì „)
        if len(sorted_ts) > MAX_CANDLE_LEN + warm_up_count:
            sorted_ts = sorted_ts[-(MAX_CANDLE_LEN + warm_up_count):]

        final_list = [",".join(candle_map[ts]) for ts in sorted_ts]

        # Redisì— ì €ì¥
        pipe = redis_client.pipeline()
        pipe.delete(key)
        for row_str in final_list:
            pipe.rpush(key, row_str)
        pipe.execute()
        
        # ì¸ë””ì¼€ì´í„° ê³„ì‚° ë° ì €ì¥
        # ë³‘í•© í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ APIì—ì„œ ì¶”ê°€ë¡œ ê°€ì ¸ì˜¤ê¸°
        if len(sorted_ts) < MIN_CANDLES_FOR_INDICATORS:
            logger.info(f"ë³‘í•© í›„ ë°ì´í„° ë¶€ì¡±, APIì—ì„œ ì¶”ê°€ ìº”ë“¤ ë¡œë“œ: {symbol} {tf_str} (í˜„ì¬: {len(sorted_ts)}ê°œ, í•„ìš”: {MIN_CANDLES_FOR_INDICATORS}ê°œ)")

            # APIì—ì„œ ì¶©ë¶„í•œ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸°
            api_candles = fetch_latest_candles(symbol, timeframe, limit=MIN_CANDLES_FOR_INDICATORS)

            if api_candles and len(api_candles) >= MIN_CANDLES_FOR_INDICATORS:
                # ìƒˆë¡œ ê°€ì ¸ì˜¨ ìº”ë“¤ ë³‘í•©
                for candle in api_candles:
                    ts = candle["timestamp"]
                    cndl_str_list = [
                        str(ts),
                        str(candle["open"]),
                        str(candle["high"]),
                        str(candle["low"]),
                        str(candle["close"]),
                        str(candle["volume"]),
                    ]
                    candle_map[ts] = cndl_str_list

                # ë‹¤ì‹œ ì •ë ¬
                sorted_ts = sorted(candle_map.keys())
                if len(sorted_ts) > MAX_CANDLE_LEN:
                    sorted_ts = sorted_ts[-MAX_CANDLE_LEN:]

                final_list = [",".join(candle_map[ts]) for ts in sorted_ts]

                # Redisì— ì €ì¥
                pipe = redis_client.pipeline()
                pipe.delete(key)
                for row_str in final_list:
                    pipe.rpush(key, row_str)
                pipe.execute()

                logger.info(f"APIì—ì„œ ì¶”ê°€ ìº”ë“¤ ë¡œë“œ ì™„ë£Œ: {symbol} {tf_str} (ì´ {len(sorted_ts)}ê°œ)")
            else:
                logger.warning(f"APIì—ì„œë„ ì¶©ë¶„í•œ ìº”ë“¤ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ: {symbol} {tf_str} (API: {len(api_candles) if api_candles else 0}ê°œ)")
                return

        # ì´ì œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ ì§€í‘œ ê³„ì‚°
        # ìº”ë“¤ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        candles = []
        for ts in sorted_ts:
            parts = candle_map[ts]
            candles.append({
                "timestamp": int(parts[0]),
                "open": float(parts[1]),
                "high": float(parts[2]),
                "low": float(parts[3]),
                "close": float(parts[4]),
                "volume": float(parts[5])
            })

        # ì¸ë””ì¼€ì´í„° ê³„ì‚° (ì „ì²´ ë°ì´í„°ë¡œ ê³„ì‚°)
        candles_with_ind = compute_all_indicators(candles, rsi_period=14, atr_period=14)

        # auto_trend_state ì¶”ê°€ (Pine Script 'ìë™' ëª¨ë“œìš©)
        auto_trend_tf_str = get_auto_trend_timeframe(tf_str)
        auto_trend_candles = _get_candles_from_redis_for_auto_trend(symbol, auto_trend_tf_str)
        if auto_trend_candles and len(auto_trend_candles) >= 30:
            candles_with_ind = add_auto_trend_state_to_candles(
                candles_with_ind,
                auto_trend_candles,
                current_timeframe_minutes=timeframe
            )
            logger.debug(f"auto_trend_state ê³„ì‚° ì™„ë£Œ: {symbol} {tf_str} (auto_trend_tf: {auto_trend_tf_str})")
        else:
            # auto_trend ìº”ë“¤ì´ ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ ì„¤ì •
            for cndl in candles_with_ind:
                cndl["auto_trend_state"] = 0
            logger.debug(f"auto_trend ìº”ë“¤ ë¶€ì¡±, auto_trend_state=0ìœ¼ë¡œ ì„¤ì •: {symbol} {tf_str}")

        # warm_up_countê°€ ì§€ì •ëœ ê²½ìš°, ì²˜ìŒ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ ì œì™¸
        if warm_up_count > 0 and len(candles_with_ind) > warm_up_count:
            logger.info(f"Warm-up ìº”ë“¤ ì œì™¸: {symbol} {tf_str} - ì²˜ìŒ {warm_up_count}ê°œ ì œì™¸, {len(candles_with_ind) - warm_up_count}ê°œ ì €ì¥")
            candles_with_ind = candles_with_ind[warm_up_count:]  # ì²˜ìŒ warm_up_countê°œ ì œì™¸

        # í•œêµ­ ì‹œê°„ ì¶”ê°€
        for cndl in candles_with_ind:
            utc_dt = datetime.fromtimestamp(cndl["timestamp"], UTC)
            seoul_tz = pytz.timezone("Asia/Seoul")
            dt_seoul = utc_dt.replace(tzinfo=pytz.utc).astimezone(seoul_tz)
            cndl["human_time"] = utc_dt.strftime("%Y-%m-%d %H:%M:%S")
            cndl["human_time_kr"] = dt_seoul.strftime("%Y-%m-%d %H:%M:%S")

        # ì¸ë””ì¼€ì´í„° í¬í•¨ ìº”ë“¤ ì €ì¥
        save_candles_with_indicators(symbol, tf_str, candles_with_ind)

        logger.debug(f"ìº”ë“¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {symbol} {tf_str} - ì´ {len(candles_with_ind)}ê°œ ìº”ë“¤ (warm-up {warm_up_count}ê°œ ì œì™¸)")
    
    except Exception as e:
        logger.error(f"ìº”ë“¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {symbol} {tf_str} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CandleDataUpdateError",
            severity="ERROR",
            symbol=symbol,
            metadata={"timeframe": tf_str, "component": "integrated_data_collector.update_candle_data"}
        )

def save_candles_with_indicators(symbol, tf_str, candles_with_ind):
    """ì¸ë””ì¼€ì´í„°ê°€ í¬í•¨ëœ ìº”ë“¤ ë°ì´í„° ì €ì¥"""
    key = f"candles_with_indicators:{symbol}:{tf_str}"

    try:
        # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        existing_list = redis_client.lrange(key, 0, -1)
        candle_map = {}

        for item in existing_list:
            try:
                obj = json.loads(item)
                if "timestamp" in obj:
                    candle_map[obj["timestamp"]] = obj
            except Exception as e:
                pass

        # ìƒˆ ë°ì´í„° ë³‘í•© (ê¸°ì¡´ ë°ì´í„°ëŠ” ë®ì–´ì“°ì§€ ì•ŠìŒ - warm-upìœ¼ë¡œ ê³„ì‚°ëœ ì •í™•í•œ ë°ì´í„° ë³´ì¡´)
        for cndl in candles_with_ind:
            ts = cndl["timestamp"]
            if ts not in candle_map:  # ìƒˆë¡œìš´ timestampë§Œ ì¶”ê°€
                candle_map[ts] = cndl

        # ì •ë ¬ í›„ ì €ì¥ (ìµœëŒ€ MAX_CANDLE_LENê°œë§Œ ìœ ì§€)
        sorted_ts = sorted(candle_map.keys())
        if len(sorted_ts) > MAX_CANDLE_LEN:
            sorted_ts = sorted_ts[-MAX_CANDLE_LEN:]

        # Redisì— ì €ì¥
        with redis_client.pipeline() as pipe:
            pipe.delete(key)
            for ts in sorted_ts:
                row_json = json.dumps(candle_map[ts])
                pipe.rpush(key, row_json)
            pipe.execute()

        # CandlesDBì—ë„ ì €ì¥ (ë¹„ë™ê¸°ì ìœ¼ë¡œ, ì‹¤íŒ¨í•´ë„ RedisëŠ” ì˜í–¥ ì—†ìŒ)
        if candlesdb_writer.enabled:
            try:
                # timeframe ë³€í™˜ (tf_str: "1m", "15m", "1h" ë“± â†’ minutes)
                timeframe_minutes = REVERSE_TF_MAP.get(tf_str, 1)  # ê¸°ë³¸ê°’ 1ë¶„

                # ìƒˆë¡œ ì¶”ê°€ëœ ìº”ë“¤ë§Œ CandlesDBì— ì €ì¥
                new_candles = [candle_map[ts] for ts in sorted_ts]
                candlesdb_writer.upsert_candles(symbol, timeframe_minutes, new_candles)
            except Exception as db_e:
                logger.warning(f"CandlesDB ì €ì¥ ì‹¤íŒ¨ (RedisëŠ” ì„±ê³µ): {symbol} {tf_str} - {db_e}")

        ## ìµœì‹  ìº”ë“¤ ë”°ë¡œ ì €ì¥ #<-- ì´ê±´, ì§€í‘œë¥¼ ê³„ì‚°í•˜ë‹ˆ í•„ìš”í• ì§€ ëª¨ë¥´ê² ë‹¤. ê·¸ëŸ¬ë‚˜, ì¼ë‹¨ì€, latestëŠ” ì›¹ì†Œì¼“ì—ì„œë§Œ ë‹¤ë£¨ëŠ”ê±¸ë¡œ.
        #latest_key = f"latest:{symbol}:{tf_str}"
        #latest_ts = sorted_ts[-1]
        #redis_client.set(latest_key, json.dumps(candle_map[latest_ts]))

    except Exception as e:
        logger.error(f"ì¸ë””ì¼€ì´í„° í¬í•¨ ìº”ë“¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {symbol} {tf_str} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CandleIndicatorSaveError",
            severity="ERROR",
            symbol=symbol,
            metadata={"timeframe": tf_str, "component": "integrated_data_collector.save_candles_with_indicators"}
        )

def fetch_initial_data():
    """ì´ˆê¸° ë°ì´í„° ë¡œë“œ"""
    logger.info("=== ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")

    # 1ë‹¨ê³„: í° íƒ€ì„í”„ë ˆì„ë¶€í„° ë¡œë“œ (auto_trend_state ê³„ì‚°ì„ ìœ„í•´)
    # ì˜ˆ: 5mì€ 30mì˜ auto_trend_stateê°€ í•„ìš”í•˜ë¯€ë¡œ, 30mì„ ë¨¼ì € ë¡œë“œ
    sorted_timeframes = sorted(TIMEFRAMES, reverse=True)  # [240, 60, 30, 15, 5, 3, 1]

    for symbol in SYMBOLS:
        for timeframe in sorted_timeframes:
            tf_str = TF_MAP.get(timeframe, "1m")
            key = f"{symbol}:{tf_str}"

            logger.info(f"ì´ˆê¸° ë°ì´í„° ë¡œë“œ: {key}")

            # ì •í™•í•œ ì§€í‘œ ê³„ì‚°ì„ ìœ„í•´ ì¶”ê°€ 200ê°œë¥¼ ë” ìš”ì²­ (warm-up ë°ì´í„°)
            requested_candles = MAX_CANDLE_LEN + MIN_CANDLES_FOR_INDICATORS  # 3000 + 200 = 3200
            candles = fetch_latest_candles(symbol, timeframe, limit=requested_candles)

            if candles:
                # ì§€í‘œ ê³„ì‚°ì€ ì „ì²´ ë°ì´í„°ë¡œ, ì €ì¥ì€ ìµœì‹  MAX_CANDLE_LENê°œë§Œ
                update_candle_data(symbol, timeframe, candles, warm_up_count=MIN_CANDLES_FOR_INDICATORS)
                last_candle_timestamps[key] = candles[-1]["timestamp"]
                logger.info(f"ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì„±ê³µ: {key} - {len(candles)}ê°œ ìº”ë“¤ (warm-up: {MIN_CANDLES_FOR_INDICATORS}ê°œ)")
            else:
                logger.warning(f"ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {key}")

    # 2ë‹¨ê³„: auto_trend_state ì¬ê³„ì‚° (ì´ì œ ëª¨ë“  íƒ€ì„í”„ë ˆì„ ë°ì´í„°ê°€ Redisì— ìˆìŒ)
    logger.info("=== auto_trend_state ì¬ê³„ì‚° ì‹œì‘ ===")
    for symbol in SYMBOLS:
        for timeframe in TIMEFRAMES:
            tf_str = TF_MAP.get(timeframe, "1m")
            key = f"{symbol}:{tf_str}"

            # í˜„ì¬ ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            candles_key = f"candles_with_indicators:{symbol}:{tf_str}"
            existing_list = redis_client.lrange(candles_key, 0, -1)

            if not existing_list:
                continue

            # ìº”ë“¤ íŒŒì‹±
            candles = []
            for item in existing_list:
                try:
                    item_str = item.decode('utf-8') if isinstance(item, bytes) else item
                    obj = json.loads(item_str)
                    candles.append(obj)
                except Exception:
                    pass

            if not candles:
                continue

            # auto_trend_state ê³„ì‚°
            auto_trend_tf_str = get_auto_trend_timeframe(tf_str)
            auto_trend_candles = _get_candles_from_redis_for_auto_trend(symbol, auto_trend_tf_str)

            if auto_trend_candles and len(auto_trend_candles) >= 30:
                candles = add_auto_trend_state_to_candles(
                    candles,
                    auto_trend_candles,
                    current_timeframe_minutes=timeframe
                )
                logger.info(f"auto_trend_state ì¬ê³„ì‚° ì™„ë£Œ: {key} (auto_trend_tf: {auto_trend_tf_str})")

                # Redisì— ì—…ë°ì´íŠ¸
                with redis_client.pipeline() as pipe:
                    pipe.delete(candles_key)
                    for cndl in candles:
                        pipe.rpush(candles_key, json.dumps(cndl))
                    pipe.execute()

                # CandlesDBì—ë„ ì—…ë°ì´íŠ¸
                if candlesdb_writer.enabled:
                    try:
                        candlesdb_writer.upsert_candles(symbol, timeframe, candles)
                    except Exception as db_e:
                        logger.warning(f"CandlesDB auto_trend_state ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {key} - {db_e}")
            else:
                logger.warning(f"auto_trend ìº”ë“¤ ë¶€ì¡±, ì¬ê³„ì‚° ë¶ˆê°€: {key} (í•„ìš”: {auto_trend_tf_str})")

    logger.info("=== auto_trend_state ì¬ê³„ì‚° ì™„ë£Œ ===")

    # ì´ˆê¸° ë¡œë“œ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì •
    initial_data_loaded.set()
    logger.info("=== ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì™„ë£Œ ===")





def update_current_candle(symbol, timeframe):
    """í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ ì—…ë°ì´íŠ¸"""
    tf_str = TF_MAP.get(timeframe, "1m")
    key = f"{symbol}:{tf_str}"
    
    try:
        # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸° (limit=2ë¡œ ì„¤ì •í•˜ì—¬ í˜„ì¬ + ì§ì „ ìº”ë“¤ í™•ë³´)
        recent_candles = fetch_latest_candles(symbol, timeframe, limit=2, include_current=True)
        
        if not recent_candles:
            logger.warning(f"í˜„ì¬ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {key}")
            return
        
        # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ ì°¾ê¸° (ë§ˆì§€ë§‰ ìº”ë“¤ì´ í˜„ì¬ ì§„í–‰ ì¤‘ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
        current_candle = None
        current_time = int(time.time())
        
        for candle in reversed(recent_candles):  # ìµœì‹  ìº”ë“¤ë¶€í„° í™•ì¸
            if (candle["timestamp"] + timeframe * 60) > current_time:
                current_candle = candle
                break
        
        if not current_candle:
            logger.warning(f"í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {key}")
            return
        
        # ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ ì •ë³´ Redisì— ì €ì¥
        current_key = f"current_candle:{symbol}:{tf_str}"
        
        # í˜„ì¬ ì‹œê° ì •ë³´ ì¶”ê°€
        utc_dt = datetime.now(UTC)
        seoul_tz = pytz.timezone("Asia/Seoul")
        dt_seoul = utc_dt.astimezone(seoul_tz)
        
        current_candle["update_time"] = utc_dt.strftime("%Y-%m-%d %H:%M:%S")
        current_candle["update_time_kr"] = dt_seoul.strftime("%Y-%m-%d %H:%M:%S")
        
        redis_client.set(current_key, json.dumps(current_candle))
        logger.debug(f"í˜„ì¬ ì§„í–‰ ìº”ë“¤ ì—…ë°ì´íŠ¸: {key} - O:{current_candle['open']} H:{current_candle['high']} L:{current_candle['low']} C:{current_candle['close']}")
        
        # ìµœì‹  ìº”ë“¤ í‚¤ë„ ì—…ë°ì´íŠ¸
        latest_key = f"latest:{symbol}:{tf_str}"
        redis_client.set(latest_key, json.dumps(current_candle))
        
        # ì¸ë””ì¼€ì´í„° í¬í•¨ ë²„ì „ë„ ì—…ë°ì´íŠ¸
        update_current_candle_with_indicators(symbol, timeframe, current_candle)
    
    except Exception as e:
        logger.error(f"í˜„ì¬ ìº”ë“¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {key} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CurrentCandleUpdateError",
            severity="WARNING",
            symbol=symbol,
            metadata={"timeframe": tf_str, "component": "integrated_data_collector.update_current_candle"}
        )

def update_current_candle_with_indicators(symbol, timeframe, current_candle):
    """í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ì— ì¸ë””ì¼€ì´í„° ê³„ì‚°í•˜ì—¬ ì—…ë°ì´íŠ¸"""
    tf_str = TF_MAP.get(timeframe, "1m")
    key = f"candles_with_indicators:{symbol}:{tf_str}"

    try:
        # ê¸°ì¡´ ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        candle_key = f"candles:{symbol}:{tf_str}"
        existing_data = redis_client.lrange(candle_key, 0, -1)
        existing_count = len(existing_data) if existing_data else 0

        # Redisì— ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ APIì—ì„œ ì¶”ê°€ë¡œ ê°€ì ¸ì˜¤ê¸°
        if existing_count < MIN_CANDLES_FOR_INDICATORS:
            logger.info(f"Redis ë°ì´í„° ë¶€ì¡±, APIì—ì„œ ì¶”ê°€ ìº”ë“¤ ë¡œë“œ: {symbol} {tf_str} (í˜„ì¬: {existing_count}ê°œ, ëª©í‘œ: {MIN_CANDLES_FOR_INDICATORS}ê°œ)")

            # APIì—ì„œ ì¶©ë¶„í•œ ìº”ë“¤ ê°€ì ¸ì˜¤ê¸°
            api_candles = fetch_latest_candles(symbol, timeframe, limit=MIN_CANDLES_FOR_INDICATORS)

            if not api_candles or len(api_candles) < MIN_CANDLES_FOR_INDICATORS:
                logger.warning(f"APIì—ì„œë„ ì¶©ë¶„í•œ ìº”ë“¤ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ: {symbol} {tf_str} (API: {len(api_candles) if api_candles else 0}ê°œ)")
                return

            # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ Redisì— ì €ì¥ (ì§€í‘œëŠ” ë‚˜ì¤‘ì— ê³„ì‚°)
            update_candle_data(symbol, timeframe, api_candles)

            # Redisì—ì„œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
            existing_data = redis_client.lrange(candle_key, 0, -1)
            existing_count = len(existing_data) if existing_data else 0

            if existing_count < MIN_CANDLES_FOR_INDICATORS:
                logger.warning(f"Redis ì—…ë°ì´íŠ¸ í›„ì—ë„ ë°ì´í„° ë¶€ì¡±: {symbol} {tf_str} (í˜„ì¬: {existing_count}ê°œ)")
                return

            logger.info(f"APIì—ì„œ ìº”ë“¤ ë¡œë“œ ì™„ë£Œ: {symbol} {tf_str} ({existing_count}ê°œ)")
        
        # ìº”ë“¤ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        candles = []
        for item in existing_data:
            # Redis returns bytes, decode to string first
            item_str = item.decode('utf-8') if isinstance(item, bytes) else item
            parts = item_str.split(",")
            ts = int(parts[0])
            candles.append({
                "timestamp": ts,
                "open": float(parts[1]),
                "high": float(parts[2]),
                "low": float(parts[3]),
                "close": float(parts[4]),
                "volume": float(parts[5])
            })
        
        # í˜„ì¬ ìº”ë“¤ ì¶”ê°€ ë˜ëŠ” ì—…ë°ì´íŠ¸
        current_ts = current_candle["timestamp"]
        found = False
        
        for i, candle in enumerate(candles):
            if candle["timestamp"] == current_ts:
                candles[i] = current_candle
                found = True
                break
        
        if not found:
            candles.append(current_candle)
            candles.sort(key=lambda x: x["timestamp"])
        
        # ì¸ë””ì¼€ì´í„° ê³„ì‚°
        candles_with_ind = compute_all_indicators(candles, rsi_period=14, atr_period=14)

        # auto_trend_state ì¶”ê°€ (Pine Script 'ìë™' ëª¨ë“œìš©)
        auto_trend_tf_str = get_auto_trend_timeframe(tf_str)
        auto_trend_candles = _get_candles_from_redis_for_auto_trend(symbol, auto_trend_tf_str)
        if auto_trend_candles and len(auto_trend_candles) >= 30:
            candles_with_ind = add_auto_trend_state_to_candles(
                candles_with_ind,
                auto_trend_candles,
                current_timeframe_minutes=timeframe
            )
        else:
            # auto_trend ìº”ë“¤ì´ ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ ì„¤ì •
            for cndl in candles_with_ind:
                cndl["auto_trend_state"] = 0

        # ê¸°ì¡´ ì¸ë””ì¼€ì´í„° ë°ì´í„° ë¡œë“œ
        existing_ind_list = redis_client.lrange(key, 0, -1)
        candle_ind_map = {}
        
        for item in existing_ind_list:
            try:
                obj = json.loads(item)
                if "timestamp" in obj:
                    candle_ind_map[obj["timestamp"]] = obj
            except Exception as e:
                pass
        
        # ìƒˆ ì¸ë””ì¼€ì´í„° ë°ì´í„° ë³‘í•©
        for candle in candles_with_ind:
            ts = candle["timestamp"]
            
            # í•œêµ­ ì‹œê°„ ì¶”ê°€
            utc_dt = datetime.fromtimestamp(ts, UTC)
            seoul_tz = pytz.timezone("Asia/Seoul")
            dt_seoul = utc_dt.replace(tzinfo=pytz.utc).astimezone(seoul_tz)
            candle["human_time"] = utc_dt.strftime("%Y-%m-%d %H:%M:%S")
            candle["human_time_kr"] = dt_seoul.strftime("%Y-%m-%d %H:%M:%S")
            
            # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ì¸ ê²½ìš° ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ê°€
            if ts == current_ts:
                utc_now = datetime.now(UTC)
                seoul_now = utc_now.astimezone(seoul_tz)
                candle["update_time"] = utc_now.strftime("%Y-%m-%d %H:%M:%S")
                candle["update_time_kr"] = seoul_now.strftime("%Y-%m-%d %H:%M:%S")
                candle["is_current"] = True
            
            candle_ind_map[ts] = candle
        
        # ì •ë ¬ í›„ ì €ì¥ (ìµœëŒ€ MAX_CANDLE_LENê°œë§Œ ìœ ì§€)
        sorted_ts = sorted(candle_ind_map.keys())
        if len(sorted_ts) > MAX_CANDLE_LEN:
            sorted_ts = sorted_ts[-MAX_CANDLE_LEN:]
        
        # Redisì— ì €ì¥
        with redis_client.pipeline() as pipe:
            pipe.delete(key)
            for ts in sorted_ts:
                row_json = json.dumps(candle_ind_map[ts])
                pipe.rpush(key, row_json)
            pipe.execute()
        
        # í˜„ì¬ ìº”ë“¤ì˜ ì¸ë””ì¼€ì´í„° ê°’ ì°¾ê¸°
        current_with_ind = candle_ind_map.get(current_ts)

        if current_with_ind:
            # í˜„ì¬ ìº”ë“¤ ë³„ë„ ì €ì¥
            current_ind_key = f"current_candle_with_indicators:{symbol}:{tf_str}"
            redis_client.set(current_ind_key, json.dumps(current_with_ind))

            # ìµœì‹  ìº”ë“¤ í‚¤ë„ ì—…ë°ì´íŠ¸
            latest_ind_key = f"latest_with_indicators:{symbol}:{tf_str}"
            redis_client.set(latest_ind_key, json.dumps(current_with_ind))

            # CandlesDBì—ë„ í˜„ì¬ ìº”ë“¤ ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„ upsert)
            if candlesdb_writer.enabled:
                try:
                    timeframe_minutes = REVERSE_TF_MAP.get(tf_str, 1)
                    candlesdb_writer.upsert_single_candle(symbol, timeframe_minutes, current_with_ind)
                except Exception as db_e:
                    logger.debug(f"CandlesDB í˜„ì¬ ìº”ë“¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {symbol} {tf_str} - {db_e}")

            logger.debug(f"í˜„ì¬ ì§„í–‰ ìº”ë“¤ ì¸ë””ì¼€ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {symbol} {tf_str}")
        else:
            logger.warning(f"í˜„ì¬ ìº”ë“¤ì˜ ì¸ë””ì¼€ì´í„° ê³„ì‚° ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {symbol} {tf_str}")
    
    except Exception as e:
        logger.error(f"í˜„ì¬ ìº”ë“¤ ì¸ë””ì¼€ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {symbol} {tf_str} - {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="CurrentCandleIndicatorUpdateError",
            severity="WARNING",
            symbol=symbol,
            metadata={"timeframe": tf_str, "component": "integrated_data_collector.update_current_candle_with_indicators"}
        )


def polling_worker():
    """í´ë§ ì›Œì»¤ í•¨ìˆ˜"""
    logger.info("í´ë§ ì›Œì»¤ ì‹œì‘")

    try:
        # ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        logger.info("ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        initial_data_loaded.wait()
        logger.info("ì´ˆê¸° ë°ì´í„° ë¡œë“œ ì™„ë£Œ í™•ì¸, í´ë§ ì‹œì‘")

        # ì´ˆê¸°í™”
        for symbol in SYMBOLS:
            for timeframe in TIMEFRAMES:
                tf_str = TF_MAP.get(timeframe, "1m")
                key = f"{symbol}:{tf_str}"
                last_check_times[key] = 0

        # Health Check íƒ€ì´ë¨¸
        last_health_check = time.time()
        health_check_interval = 300  # 5ë¶„ë§ˆë‹¤ health check
        stats_log_interval = 600  # 10ë¶„ë§ˆë‹¤ í†µê³„ ë¡œê·¸

        # Redis ëª¨ë‹ˆí„°ë§ ì¹´ìš´í„°
        redis_success_count = 0
        redis_failure_count = 0
        redis_last_failure_time = None

        while not shutdown_event.is_set():
            current_time = time.time()

            # Health Check (5ë¶„ë§ˆë‹¤)
            if current_time - last_health_check >= health_check_interval:
                # CandlesDB Health Check
                logger.debug("ğŸ¥ CandlesDB health check ì‹¤í–‰...")
                candlesdb_writer.health_check()

                # Redis Health Check
                logger.debug("ğŸ¥ Redis health check ì‹¤í–‰...")
                try:
                    if redis_manager.ping_sync():
                        redis_success_count += 1
                        logger.debug("âœ… Redis health check: OK")
                    else:
                        redis_failure_count += 1
                        redis_last_failure_time = current_time
                        logger.warning("âš ï¸ Redis health check failed: ping returned False")
                        # ì¬ì—°ê²° ì‹œë„
                        logger.info("ğŸ”„ Redis ì¬ì—°ê²° ì‹œë„...")
                        global redis_client
                        redis_client = redis_manager.get_connection()
                except Exception as e:
                    redis_failure_count += 1
                    redis_last_failure_time = current_time
                    logger.error(f"âŒ Redis health check failed: {e}")
                    # ì¬ì—°ê²° ì‹œë„
                    try:
                        logger.info("ğŸ”„ Redis ì¬ì—°ê²° ì‹œë„...")
                        redis_client = redis_manager.get_connection()
                        if redis_manager.ping_sync():
                            logger.info("âœ… Redis ì¬ì—°ê²° ì„±ê³µ!")
                    except Exception as reconnect_e:
                        logger.error(f"âŒ Redis ì¬ì—°ê²° ì‹¤íŒ¨: {reconnect_e}")

                last_health_check = current_time

                # í†µê³„ ë¡œê·¸ (10ë¶„ë§ˆë‹¤)
                if current_time % stats_log_interval < health_check_interval:
                    candlesdb_writer.log_stats()

                    # Redis í†µê³„ ë¡œê·¸
                    total_checks = redis_success_count + redis_failure_count
                    success_rate = (redis_success_count / total_checks * 100) if total_checks > 0 else 0.0
                    logger.info(
                        f"ğŸ“Š Redis Stats: "
                        f"success={redis_success_count}, "
                        f"failure={redis_failure_count}, "
                        f"rate={success_rate:.1f}%"
                    )
            
            for symbol in SYMBOLS:
                for timeframe in TIMEFRAMES:
                    tf_str = TF_MAP.get(timeframe, "1m")
                    key = f"{symbol}:{tf_str}"
                    
                    # ê° íƒ€ì„í”„ë ˆì„ë³„ ì—…ë°ì´íŠ¸ ì£¼ê¸° ê³„ì‚°
                    update_interval = calculate_update_interval(timeframe)
                    
                    # ë§ˆì§€ë§‰ ì²´í¬ ì‹œê°„ ì´í›„ ì¶©ë¶„í•œ ì‹œê°„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸
                    last_check = last_check_times.get(key, 0)
                    
                    # ë°” ì¢…ë£Œ ì‹œì  ì²´í¬
                    if is_bar_end(current_time, timeframe):
                        # ë°” ì¢…ë£Œ ì‹œì ì—ëŠ” ì™„ë£Œëœ ìº”ë“¤ ì—…ë°ì´íŠ¸ (5ì´ˆ ê°„ê²©ìœ¼ë¡œ ì²´í¬)
                        if current_time - last_check >= 5:
                            logger.debug(f"ë°” ì¢…ë£Œ ê°ì§€: {key} - ë°ì´í„° í´ë§ ì‹œì‘")
                            
                            # ìµœì‹  ìº”ë“¤ 100ê°œ ê°€ì ¸ì˜¤ê¸°
                            candles = fetch_latest_candles(symbol, timeframe, limit=POLLING_CANDLES)
                            
                            if candles:
                                # ê°­ ì²´í¬ ë° ë°ì´í„° ì—…ë°ì´íŠ¸
                                check_and_fill_gap(symbol, timeframe)
                                update_candle_data(symbol, timeframe, candles)
                            
                            # ë§ˆì§€ë§‰ ì²´í¬ ì‹œê°„ ì—…ë°ì´íŠ¸
                            last_check_times[key] = current_time
                    else:
                        # ì¼ë°˜ ì‹œì ì—ëŠ” íƒ€ì„í”„ë ˆì„ë³„ ê³„ì‚°ëœ ê°„ê²©ìœ¼ë¡œ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìº”ë“¤ ì—…ë°ì´íŠ¸
                        if current_time - last_check >= update_interval:
                            logger.debug(f"í˜„ì¬ ì§„í–‰ ìº”ë“¤ ì—…ë°ì´íŠ¸ ì‹¤í–‰: {key} (ê°„ê²©: {update_interval}ì´ˆ)")
                            update_current_candle(symbol, timeframe)
                            
                            # ë§ˆì§€ë§‰ ì²´í¬ ì‹œê°„ ì—…ë°ì´íŠ¸
                            last_check_times[key] = current_time
            
            # ì ì‹œ ëŒ€ê¸° (CPU ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°)
            time.sleep(1)
    
    except Exception as e:
        logger.error(f"í´ë§ ì›Œì»¤ ì˜¤ë¥˜: {e}", exc_info=True)
        # errordb ë¡œê¹…
        from HYPERRSI.src.utils.error_logger import log_error_to_db
        log_error_to_db(
            error=e,
            error_type="PollingWorkerError",
            severity="CRITICAL",
            metadata={"component": "integrated_data_collector.polling_worker"}
        )
    finally:
        logger.info("í´ë§ ì›Œì»¤ ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        logger.info("=== í´ë§ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹œì‘ ===")
        
        # ì´ˆê¸° ë°ì´í„° ë¡œë“œ
        fetch_initial_data()
        
        # í´ë§ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        polling_thread = threading.Thread(target=polling_worker, daemon=True)
        polling_thread.start()
        
        # ë©”ì¸ ìŠ¤ë ˆë“œëŠ” ì¢…ë£Œ ì‹ í˜¸ ëŒ€ê¸°
        try:
            while polling_thread.is_alive():
                time.sleep(1)
                
                # ì¢…ë£Œ ì²´í¬
                if shutdown_event.is_set():
                    logger.info("ì¢…ë£Œ ì‹ í˜¸ ê°ì§€")
                    break
        
        except KeyboardInterrupt:
            logger.info("í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ ê°ì§€, ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤...")
            shutdown_event.set()
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        polling_thread.join(timeout=5)
        logger.info("í´ë§ ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œë¨")
    
    except Exception as e:
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
    
    finally:
        logger.info("=== í´ë§ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ê¸° ì¢…ë£Œ ===")

if __name__ == "__main__":
    main()