"""
ìœ ì§€ë³´ìˆ˜ ê´€ë ¨ Celery íƒœìŠ¤í¬
ë¡œê·¸ ì •ë¦¬, ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ë“± ì£¼ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import gzip
import os
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any

from celery import shared_task
from celery.utils.log import get_task_logger

from HYPERRSI.src.core.logger import error_logger

logger = get_task_logger(__name__)


@shared_task(name='maintenance_tasks.cleanup_old_logs')
def cleanup_old_logs() -> Dict[str, Any]:
    """
    ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

    - 7ì¼ ì´ìƒ ëœ ë¡œê·¸ íŒŒì¼ ì••ì¶•
    - 30ì¼ ì´ìƒ ëœ ì••ì¶• íŒŒì¼ ì‚­ì œ
    - 100MB ì´ìƒ ë¡œê·¸ íŒŒì¼ ì¦‰ì‹œ ì••ì¶•

    Returns:
        Dict[str, Any]: ì •ë¦¬ ê²°ê³¼ í†µê³„
    """
    try:
        logger.info("ğŸ§¹ ë¡œê·¸ ì •ë¦¬ ì‘ì—… ì‹œì‘")

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        base_dir = Path(__file__).parent.parent.parent
        log_dir = base_dir / 'logs'

        if not log_dir.exists():
            logger.warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {log_dir}")
            return {'success': False, 'error': 'Log directory not found'}

        # ì„¤ì •
        COMPRESS_DAYS = 7
        DELETE_DAYS = 30
        MAX_SIZE_MB = 100

        stats = {
            'compressed_count': 0,
            'deleted_count': 0,
            'space_freed_mb': 0,
            'errors': []
        }

        # 1. í° ë¡œê·¸ íŒŒì¼ ì¦‰ì‹œ ì••ì¶• (100MB ì´ìƒ)
        logger.info(f"ğŸ“¦ 1ë‹¨ê³„: {MAX_SIZE_MB}MB ì´ìƒ ë¡œê·¸ íŒŒì¼ ì••ì¶•")
        for log_file in log_dir.rglob('*.log'):
            try:
                size_mb = log_file.stat().st_size / (1024 * 1024)
                if size_mb >= MAX_SIZE_MB:
                    logger.info(f"  ì••ì¶• ì¤‘: {log_file.name} ({size_mb:.1f}MB)")
                    compress_file(log_file)
                    stats['compressed_count'] += 1
                    stats['space_freed_mb'] += size_mb * 0.7  # ì•½ 70% ì••ì¶•ë¥ 
            except Exception as e:
                error_msg = f"íŒŒì¼ ì••ì¶• ì‹¤íŒ¨ ({log_file.name}): {str(e)}"
                logger.error(error_msg)
                stats['errors'].append(error_msg)

        # 2. ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì••ì¶• (7ì¼ ì´ìƒ)
        logger.info(f"ğŸ“¦ 2ë‹¨ê³„: {COMPRESS_DAYS}ì¼ ì´ìƒ ë¡œê·¸ íŒŒì¼ ì••ì¶•")
        cutoff_date = datetime.now() - timedelta(days=COMPRESS_DAYS)

        for log_file in log_dir.rglob('*.log'):
            try:
                mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                if mtime < cutoff_date:
                    size_mb = log_file.stat().st_size / (1024 * 1024)
                    logger.debug(f"  ì••ì¶• ì¤‘: {log_file.name} ({size_mb:.1f}MB)")
                    compress_file(log_file)
                    stats['compressed_count'] += 1
                    stats['space_freed_mb'] += size_mb * 0.7
            except Exception as e:
                error_msg = f"íŒŒì¼ ì••ì¶• ì‹¤íŒ¨ ({log_file.name}): {str(e)}"
                logger.error(error_msg)
                stats['errors'].append(error_msg)

        # 3. ì˜¤ë˜ëœ ì••ì¶• íŒŒì¼ ì‚­ì œ (30ì¼ ì´ìƒ)
        logger.info(f"ğŸ—‘ï¸  3ë‹¨ê³„: {DELETE_DAYS}ì¼ ì´ìƒ ì••ì¶• íŒŒì¼ ì‚­ì œ")
        delete_cutoff = datetime.now() - timedelta(days=DELETE_DAYS)

        for gz_file in log_dir.rglob('*.log.gz'):
            try:
                mtime = datetime.fromtimestamp(gz_file.stat().st_mtime)
                if mtime < delete_cutoff:
                    size_mb = gz_file.stat().st_size / (1024 * 1024)
                    logger.debug(f"  ì‚­ì œ ì¤‘: {gz_file.name} ({size_mb:.1f}MB)")
                    gz_file.unlink()
                    stats['deleted_count'] += 1
                    stats['space_freed_mb'] += size_mb
            except Exception as e:
                error_msg = f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({gz_file.name}): {str(e)}"
                logger.error(error_msg)
                stats['errors'].append(error_msg)

        # 4. ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬
        logger.info("ğŸ§¹ 4ë‹¨ê³„: ë¹ˆ ë””ë ‰í† ë¦¬ ì •ë¦¬")
        empty_dirs_removed = 0
        for dirpath in log_dir.rglob('*'):
            if dirpath.is_dir() and not any(dirpath.iterdir()):
                try:
                    dirpath.rmdir()
                    empty_dirs_removed += 1
                    logger.debug(f"  ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ: {dirpath.name}")
                except Exception as e:
                    logger.warning(f"ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨ ({dirpath.name}): {str(e)}")

        stats['empty_dirs_removed'] = empty_dirs_removed

        # ìµœì¢… ê²°ê³¼
        logger.info(
            f"âœ… ë¡œê·¸ ì •ë¦¬ ì™„ë£Œ: "
            f"ì••ì¶• {stats['compressed_count']}ê°œ, "
            f"ì‚­ì œ {stats['deleted_count']}ê°œ, "
            f"ì ˆì•½ {stats['space_freed_mb']:.1f}MB"
        )

        # ì—ëŸ¬ ë¡œê±°ì—ë„ ê¸°ë¡
        error_logger.info(
            f"ë¡œê·¸ ì •ë¦¬ ì‘ì—… ì™„ë£Œ - "
            f"ì••ì¶•: {stats['compressed_count']}, "
            f"ì‚­ì œ: {stats['deleted_count']}, "
            f"ì ˆì•½: {stats['space_freed_mb']:.1f}MB"
        )

        stats['success'] = True
        return stats

    except Exception as e:
        error_msg = f"ë¡œê·¸ ì •ë¦¬ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg, exc_info=True)
        error_logger.error(error_msg, exc_info=True)
        return {
            'success': False,
            'error': str(e),
            'compressed_count': stats.get('compressed_count', 0),
            'deleted_count': stats.get('deleted_count', 0),
            'space_freed_mb': stats.get('space_freed_mb', 0)
        }


def compress_file(file_path: Path) -> None:
    """
    íŒŒì¼ì„ gzipìœ¼ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.

    Args:
        file_path: ì••ì¶•í•  íŒŒì¼ ê²½ë¡œ
    """
    gz_path = Path(str(file_path) + '.gz')

    # ì´ë¯¸ ì••ì¶• íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
    if gz_path.exists():
        logger.debug(f"ì••ì¶• íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {gz_path.name}")
        return

    try:
        with open(file_path, 'rb') as f_in:
            with gzip.open(gz_path, 'wb', compresslevel=9) as f_out:
                shutil.copyfileobj(f_in, f_out)

        # ì›ë³¸ íŒŒì¼ ì‚­ì œ
        file_path.unlink()
        logger.debug(f"ì••ì¶• ì™„ë£Œ: {file_path.name} â†’ {gz_path.name}")

    except Exception as e:
        logger.error(f"íŒŒì¼ ì••ì¶• ì¤‘ ì˜¤ë¥˜ ({file_path.name}): {str(e)}")
        # ì••ì¶• ì‹¤íŒ¨ ì‹œ ìƒì„±ëœ gz íŒŒì¼ ì‚­ì œ
        if gz_path.exists():
            try:
                gz_path.unlink()
            except:
                pass
        raise


@shared_task(name='maintenance_tasks.analyze_logs_summary')
def analyze_logs_summary(days: int = 1) -> Dict[str, Any]:
    """
    ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        days: ë¶„ì„í•  ê¸°ê°„ (ì¼)

    Returns:
        Dict[str, Any]: ë¡œê·¸ ë¶„ì„ ìš”ì•½
    """
    try:
        import json
        from collections import Counter
        from datetime import datetime, timedelta

        logger.info(f"ğŸ“Š ìµœê·¼ {days}ì¼ ë¡œê·¸ ë¶„ì„ ì‹œì‘")

        base_dir = Path(__file__).parent.parent.parent
        log_dir = base_dir / 'logs'
        orders_log = log_dir / 'orders' / 'trading_orders.log'

        if not orders_log.exists():
            logger.warning("ì£¼ë¬¸ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No order logs found'}

        cutoff_date = datetime.now() - timedelta(days=days)

        action_types = Counter()
        symbols = Counter()
        total_orders = 0
        errors = 0

        with open(orders_log, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    log_entry = json.loads(line)
                    log_time = datetime.fromisoformat(log_entry.get('timestamp', ''))

                    if log_time >= cutoff_date:
                        total_orders += 1
                        action_types[log_entry.get('action_type', 'unknown')] += 1
                        symbols[log_entry.get('symbol', 'unknown')] += 1

                        if log_entry.get('level') == 'ERROR':
                            errors += 1

                except (json.JSONDecodeError, ValueError, KeyError):
                    continue

        summary = {
            'success': True,
            'period_days': days,
            'total_orders': total_orders,
            'errors': errors,
            'top_actions': dict(action_types.most_common(5)),
            'top_symbols': dict(symbols.most_common(5)),
            'timestamp': datetime.now().isoformat()
        }

        logger.info(
            f"ğŸ“Š ë¡œê·¸ ë¶„ì„ ì™„ë£Œ: "
            f"ì´ ì£¼ë¬¸ {total_orders}ê°œ, "
            f"ì—ëŸ¬ {errors}ê°œ"
        )

        return summary

    except Exception as e:
        error_msg = f"ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return {'success': False, 'error': str(e)}
